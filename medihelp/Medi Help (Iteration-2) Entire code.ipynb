{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from PIL import Image\n",
    "except ImportError:\n",
    "    import Image\n",
    "import pytesseract\n",
    "import bs4 as bs  \n",
    "from urllib.request import Request, urlopen  \n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,WhitespaceTokenizer\n",
    "import nltk\n",
    "import heapq  \n",
    "import requests\n",
    "import re\n",
    "#from lxml.html import fromstring\n",
    "#%pylab inline \n",
    "import cv2\n",
    "#from IPython.display import clear_output\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "\n",
    "def by_size(words, size):\n",
    "    return [word for word in words if len(word) >= size]\n",
    "def Remove(duplicate): \n",
    "    final_list = [] \n",
    "    for num in duplicate: \n",
    "        if num not in final_list: \n",
    "            final_list.append(num) \n",
    "    return final_list \n",
    "\n",
    "img = cv2.imread(\"mupir.jpeg\")\n",
    "#print('Original Dimensions : ',img.shape)\n",
    "scale_percent = 200 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "# resize image\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "#print('Resized Dimensions : ',resized.shape)\n",
    "\n",
    "txt =pytesseract.image_to_string(resized)\n",
    "#lst=word_tokenize(txt)\n",
    "#lst.sort(key=len) \n",
    "#lst.sort(key=lambda item: (-len(item), item))\n",
    "#print(lst)\n",
    "\n",
    "\n",
    "\n",
    "result1 = re.sub(r\"\\W\", \" \", txt, flags=re.I) \n",
    "#remove digit \n",
    "result2 = re.sub(r\"\\d\", \"\", result1) \n",
    "lst=word_tokenize(result2)\n",
    "lst.sort(key=len) \n",
    "lst.sort(key=lambda item: (-len(item), item))\n",
    "\n",
    "lst2= by_size(lst,5)\n",
    "lst3=Remove(lst2)\n",
    "#lst3\n",
    "#str0 = 'https://www.google.com/search?q='\n",
    "#str1 = 'lybrate'\n",
    "#str2= '&source=lnms'\n",
    "#for l in lst3:\n",
    "#    str1 = str1+'+'+l\n",
    "#u = str1+str2    \n",
    "#url = u.encode('utf-8')\n",
    "#url = url.decode('utf-8')\n",
    "#url = str0 + url \n",
    "#url = url[2:]\n",
    "article_text = ''\n",
    "str1 = 'https://www.google.com/search?q=lybrate'\n",
    "str2= '&source=lnms'\n",
    "\n",
    "for l in lst3:\n",
    "    str1 = str1+'+'+l\n",
    "\n",
    "#str1 = str1.lower()    \n",
    "url = str1+str2  \n",
    "\n",
    "print(url)\n",
    "\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "parsed_article = bs.BeautifulSoup(webpage,'html')\n",
    "containers = parsed_article.select(\".r a\")\n",
    "for cont in containers:\n",
    "    med = cont['href']\n",
    "    med = med[7:]\n",
    "    med = med.split(\"&\")\n",
    "    med = med[0]\n",
    "    if med[0]=='h':\n",
    "        #print(med)\n",
    "        req1 = Request(med, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage1 = urlopen(req1).read()\n",
    "        parsed_article1 = bs.BeautifulSoup(webpage1,'html')\n",
    "        #p =parsed_article1.find('div', {'class': 'body'})\n",
    "        #article_text = p.text\n",
    "        points =parsed_article1.findAll('div', {'class': 'lybMar-btm--double'})\n",
    "        for p in points:\n",
    "            #print(p.text)\n",
    "            article_text += p.text\n",
    "            \n",
    "        break\n",
    "print(article_text)\n",
    "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)  \n",
    "article_text = re.sub(r'\\s+', ' ', article_text)  \n",
    "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )  \n",
    "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)  \n",
    "\n",
    "sentence_list = nltk.sent_tokenize(article_text)  \n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_frequencies = {}  \n",
    "for word in nltk.word_tokenize(formatted_article_text):  \n",
    "    if word not in stopwords:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "sentence_scores = {}  \n",
    "for sent in sentence_list:  \n",
    "    for word in nltk.word_tokenize(sent.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if len(sent.split(' ')) < 30:\n",
    "                if sent not in sentence_scores.keys():\n",
    "                    sentence_scores[sent] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sent] += word_frequencies[word]\n",
    "summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "summary = ' '.join(summary_sentences)  \n",
    "\n",
    "\n",
    "print(summary)  \n",
    "\n",
    "pygame.mixer.init()\n",
    "language = 'en'\n",
    "\n",
    "tts = gTTS(text=summary, lang=language, slow=False)\n",
    "tts.save(\"voice.mp3\")\n",
    "os.system(\"voice.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
